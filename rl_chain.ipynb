{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MealPlanner:\n",
    "    def __init__(self, name: str, desc: str, difficulty: str, tags: str):\n",
    "        try:\n",
    "            self.name = name\n",
    "            self.desc = desc\n",
    "            self.diff = difficulty\n",
    "            self.tags = tags\n",
    "        except:\n",
    "            print(name)\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actions\n",
    "## examples copied from hello fresh website\n",
    "actions = []\n",
    "actions.append(MealPlanner(name=\"One-Pan Beef Enchiladas Verdes with Mexican Cheese Blend & Hot Sauce Crema\", difficulty=\"Easy\", tags=\"Spicy, Easy Cleanup, Easy Prep\", desc=\"When it comes to Mexican-style cuisine, burritos typically get all the glory. In our humble opinion, enchiladas are an unsung dinner hero. They’re technically easier-to-assemble burritos that get smothered in a delicious sauce, but they’re really so much more than that! Ours start with spiced beef and charred green pepper that get rolled up in warm tortillas. This winning combo gets topped with tangy salsa verde and cheese, then baked until bubbly and melty. Hear that? That’s the sound of the dinner bell!\"))\n",
    "actions.append(MealPlanner(name=\"Chicken & Mushroom Flatbreads with Gouda Cream Sauce & Parmesan\", difficulty=\"Easy\", tags=\"\", desc=\"Yes we love our simple cheese pizza with red sauce but tonight, move over, marinara—there’s a new sauce in town. In this recipe, crispy flatbreads are slathered with a rich, creamy gouda-mustard sauce we just can’t get enough of. We top that off with a pile of caramelized onion and earthy cremini mushrooms. Shower with Parmesan, and that’s it. Simple, satisfying, and all in 30 minutes–a dinner idea you can’t pass up!\"))\n",
    "actions.append(MealPlanner(name=\"Sweet Potato & Pepper Quesadillas with Southwest Crema & Tomato Salsa\", difficulty=\"Easy\", tags=\"Veggie\", desc=\"This quesadilla is jam-packed with flavorful roasted sweet potato and green pepper, plus two types of gooey, melty cheese (how could we choose just one?!). Of course, we’d never forget the toppings—there’s a fresh tomato salsa and dollops of spiced lime crema. Now for the fun part: piling on a little bit of everything to construct the perfect bite!\"))\n",
    "actions.append(MealPlanner(name=\"One-Pan Trattoria Tortelloni Bake with a Crispy Parmesan Panko Topping\", difficulty=\"Easy\", tags=\"Veggie, Easy Cleanup, Easy Prep\", desc=\"Think a cheesy stuffed pasta can’t get any better? What about baking it in a creamy sauce with a crispy topping? In this recipe, we toss cheese-stuffed tortelloni in an herby tomato cream sauce, then top with Parmesan and panko breadcrumbs. Once broiled, it turns into a showstopping topping that’ll earn you plenty of oohs and aahs from your lucky fellow diners.\"))\n",
    "\n",
    "actions_strs = []\n",
    "for action in actions:\n",
    "    action_str = \"title=\" + action.name + \" description=\" + action.desc + \" tags=\" + action.tags\n",
    "    action_str = action_str.replace(\":\", \"\")\n",
    "    action_str = action_str.replace(\"|\", \"\")\n",
    "    actions_strs.append(action_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# assuming LLM api keys have been set in the environment\n",
    "# can use whatever LLM you want here doesn't have to be AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    request_timeout=20,\n",
    "    max_retries=1,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm.predict('Are you ready?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### default chain with default prompt and default reward (the LLM is used to judge and rank the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl_chain\n",
    "\n",
    "chain = rl_chain.pick_best_chain.PickBest.from_llm(\n",
    "    llm=llm,\n",
    "    response_validator = rl_chain.pick_best_chain.AutoValidatePickBest(llm=llm),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is recommended to embedd the action features and the context feature\n",
    "\n",
    "inputs = {\n",
    "    \"text_to_personalize\": \"This is the weeks specialty dish, our master chefs believe you will love it!\",\n",
    "    \"actions\": rl_chain.Embed(actions_strs),\n",
    "}\n",
    "\n",
    "for _ in range(1):\n",
    "    try:\n",
    "        inputs[\"context\"] = rl_chain.Embed(\"User: Tom, Preference: Vegetarian, regular dairy is ok\")\n",
    "        response = chain.run(inputs)\n",
    "\n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"text_to_personalize\": \"This is the weeks specialty dish, our master chefs believe you will love it!\",\n",
    "    \"actions\": rl_chain.Embed(actions_strs),\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "    try:\n",
    "        inputs[\"context\"] = rl_chain.Embed(\"User: Tom, Preference: Vegetarian, regular dairy is ok\")\n",
    "        response = chain.run(inputs)\n",
    "\n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chain with custom prompt and default reward (the LLM is used to judge and rank the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_OTHER_PROMPT_TEMPLATE = \"\"\"You can use the actions that were chosen by VW like so: {selected}.\n",
    "\n",
    "And use whatever other vars you want to pass into the chain at run: {some_text}. And {some_other_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "OTHER_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"selected\", \"some_text\", \"some_other_text\"],\n",
    "    template=_OTHER_PROMPT_TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl_chain.pick_best_chain\n",
    "\n",
    "chain = rl_chain.pick_best_chain.PickBest.from_llm(\n",
    "    llm=llm,\n",
    "    model_save_dir=\"./models\",  # where to save the model checkpoints\n",
    "    prompt=OTHER_PROMPT,\n",
    "    response_validator = rl_chain.pick_best_chain.AutoValidatePickBest(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to set the context though (can also be used in the prompt) because VW will use it in the Contextual Bandit call\n",
    "\n",
    "inputs = {\n",
    "    \"some_text\": \"This is some text\",\n",
    "    \"some_other_text\": \"This is some other text\",\n",
    "    \"actions\" : [\"an action\", \"another action\", \"a third action\"]\n",
    "}\n",
    "\n",
    "for _ in range(1):\n",
    "    try:\n",
    "        inputs[\"context\"] = \"User: Tom, Preference: Vegetarian\"\n",
    "        response = chain.run(inputs)\n",
    "    \n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actions and context with multiple namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to set the context though (can also be used in the prompt) because VW will use it in the Contextual Bandit call\n",
    "# each action is a dictionary of namespace to action string\n",
    "# this example here shows that while embedding is recommended for all features, it is not required and can be customized\n",
    "action_strs_w_ns = [{\"A\":\"an action feature\", \"B\" : rl_chain.Embed(\"antoher action feature\")}, rl_chain.Embed({\"B\": \"another action\"}), {\"C\":\"a third action\"}]\n",
    "\n",
    "inputs = {\n",
    "    \"some_text\": \"This is some text\",\n",
    "    \"some_other_text\": \"This is some other text\",\n",
    "    \"actions\" : action_strs_w_ns\n",
    "}\n",
    "\n",
    "for _ in range(1):\n",
    "    try:\n",
    "        inputs[\"context\"] = {\"User\": \"Tom\", \"Preference\": rl_chain.Embed(\"Vegetarian\")}\n",
    "        response = chain.run(inputs)\n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a checkpoint to the file (overriding existing checkpoint until the chain is restarted)\n",
    "chain.save_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chain with default prompt and custom reward prompt (the LLM is used to judge and rank the response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"PLEASE RESPOND ONLY WITH A SIGNLE FLOAT AND NO OTHER TEXT EXPLANATION\\n Return the number that is given to you\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Return this number {a_number}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "REWARD_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl_chain\n",
    "\n",
    "chain = rl_chain.pick_best_chain.PickBest.from_llm(\n",
    "    llm=llm,\n",
    "    model_save_dir=\"./models\",  # where to save the model checkpoints\n",
    "    response_validator=rl_chain.pick_best_chain.AutoValidatePickBest(llm=llm, prompt=REWARD_PROMPT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to set the context though (can also be used in the prompt) because VW will use it in the Contextual Bandit call\n",
    "actions_strs = [\"an action\", \"another action\", \"a third action\"]\n",
    "inputs = {\n",
    "    \"text_to_personalize\": \"This is the weeks specialty dish, our master chefs believe you will love it!\",\n",
    "    \"actions\": actions_strs,\n",
    "}\n",
    "\n",
    "for _ in range(1):\n",
    "    try:\n",
    "        inputs[\"context\"] = \"User: Tom, Preference: Vegetarian\"\n",
    "        inputs[\"a_number\"] = 100\n",
    "        response = chain.run(inputs)\n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### other reward options\n",
    "\n",
    "- custom reward class\n",
    "- async reward setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom reward class/function is just defining another class that inherits from RewardChecker and implementing the grade_response method\n",
    "import rl_chain\n",
    "\n",
    "class CustomResponseValidator(rl_chain.ResponseValidator):\n",
    "    def grade_response(\n",
    "        self, inputs, llm_response: str\n",
    "    ) -> float:\n",
    "        # do whatever you want here, use whatever inputs you supplied and return reward\n",
    "        reward = 1.0\n",
    "        return reward\n",
    "    \n",
    "# set this in the chain during construction (response_validator=CustomResponseValidator()) and it will be auto-called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async reward\n",
    "# set check_response to False\n",
    "\n",
    "import rl_chain\n",
    "\n",
    "chain = rl_chain.pick_best_chain.PickBest.from_llm(\n",
    "    llm=llm,\n",
    "    model_save_dir=\"./models\",  # where to save the model checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_strs = [\"an action\", \"another action\", \"a third action\"]\n",
    "inputs = {\n",
    "    \"text_to_personalize\": \"This is the weeks specialty dish, our master chefs believe you will love it!\",\n",
    "    \"actions\": actions_strs,\n",
    "}\n",
    "\n",
    "# whenever you have the reward for the call, send it back to the chain to learn from\n",
    "for _ in range(1):\n",
    "    try:\n",
    "        inputs[\"context\"] = \"User: Tom, Preference: Vegetarian\"\n",
    "        response = chain.run(inputs)\n",
    "        print(response[\"response\"])\n",
    "        rr = response[\"response_result\"]\n",
    "        # cost should be None here because we turned auto-checkin off\n",
    "        print(f\"cost: {rr.cost}, action: {rr.chosen_action}, probability: {rr.chosen_action_probability}, \")\n",
    "        chain.learn_delayed_reward(reward=1.0, response_result=rr)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
