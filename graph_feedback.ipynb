{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | GPT3.5 | GPT4 |\n",
    "|----------|----------|----------|\n",
    "| GPT3.5   | 1      | 0      |\n",
    "| GPT4     | 1      | 1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# assuming LLM api keys have been set in the environment\n",
    "\n",
    "llm_35 = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    request_timeout=20,\n",
    "    max_retries=1,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm_4 = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4-32k\",\n",
    "    temperature=0,\n",
    "    request_timeout=20,\n",
    "    max_retries=1,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm_35.predict('What American cartoonist is the creator of Andy Lippincott?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_SIMPLE_PROMPT = \"\"\"{context}\"\"\"\n",
    "\n",
    "\n",
    "OTHER_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=_SIMPLE_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_feedback_chain import GraphFeedbackChain\n",
    "\n",
    "graph_chain = GraphFeedbackChain.from_llms({\"gpt-35-turbo\": llm_35, \"gpt-4\": llm_4}, prompt=OTHER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs={\"selected\": \"gpt-35-turbo\", \"context\":\"What American cartoonise is the creator of Andy Lippincott?\"}\n",
    "\n",
    "graph_chain.run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"PLEASE RESPOND ONLY WITH A SIGNLE FLOAT AND NO OTHER TEXT EXPLANATION\\n You are a VERY VERY strict judge that is called on to rank a response based on given criteria.\\\n",
    "                You must respond with your ranking by providing a single float within the range [-1, 1], -1 being very bad response and 1 being very good response.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Given the answer: {answer} to the question: {context}, how would you rate the answer of {llm_response}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "REWARD_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_chain.pick_best_chain import ContextualBanditTextEmbedder\n",
    "class GraphFeedbackTextEmbedder(ContextualBanditTextEmbedder):\n",
    "    \"\"\"Specific text embedder to the graph used in the example notebook, not for general consumption\"\"\"\n",
    "    def to_vw_format(self, inputs, cb_label=None) -> str:\n",
    "        if cb_label:\n",
    "            chosen_action, cost, prob = cb_label\n",
    "\n",
    "        context = inputs.get('context')\n",
    "        actions = inputs.get('actions')\n",
    "        graph = inputs.get('graph')\n",
    "\n",
    "        context_emb = self.embed_context(context) if context else None\n",
    "        action_embs = self.embed_actions(actions) if actions else None\n",
    "\n",
    "        if not context_emb or not action_embs:\n",
    "            raise ValueError(\"Context and actions must be provided in the inputs dictionary\")\n",
    "\n",
    "        example_string = \"\"\n",
    "        example_string += f\"shared graph {graph}\"\n",
    "        # example_string += f\"shared \"\n",
    "        for ns, context in context_emb.items():\n",
    "            example_string += f\"|{ns} {context} \"\n",
    "        example_string += \"\\n\"\n",
    "\n",
    "        for i, action in enumerate(action_embs):\n",
    "            if cb_label:\n",
    "                if chosen_action == i:\n",
    "                    example_string += f\"{chosen_action}:{cost}:{1} \"\n",
    "                elif chosen_action == 1:\n",
    "                    example_string += f\"{chosen_action}:{cost}:{1} \"\n",
    "            for ns, action_embedding in action.items():\n",
    "                example_string += f\"|{ns} {action_embedding} \"\n",
    "            example_string += \"\\n\"\n",
    "        # Strip the last newline\n",
    "        return example_string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rl_chain\n",
    "\n",
    "chain = rl_chain.pick_best_chain.PickBest.from_chain(\n",
    "    llm_chain=graph_chain,\n",
    "    model_save_dir=\"./gf_models_actual_gf\",  # where to save the model checkpoints\n",
    "    response_validator = rl_chain.pick_best_chain.AutoValidatePickOne(llm=llm_35, prompt=REWARD_PROMPT),\n",
    "    vw_cmd=['--cb_explore_adf', '--quiet', '--interactions=::', '--coin', '--graph_feedback'],\n",
    "    prompt=OTHER_PROMPT,\n",
    "    text_embedder=GraphFeedbackTextEmbedder('bert-base-nli-mean-tokens'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = {\"actions\":[\"gpt-35-turbo\", \"gpt-4\"], \"graph\": \"0,0,1 0,1,0 1,0,1 1,1,1\"}\n",
    "\n",
    "for q, a in [(\"which country is john berry from\", \"United States of America country in North America\")]:\n",
    "    try:\n",
    "        inputs[\"context\"] = q\n",
    "        inputs[\"answer\"] = a\n",
    "        r = chain.run(inputs)\n",
    "        resp = r[\"response\"]\n",
    "        chosen_action = r[\"response_result\"].chosen_action\n",
    "        cost = r[\"response_result\"].cost\n",
    "        print(f\"m: {chosen_action} -- answer: {a} -- response: {resp}\")\n",
    "        print(r[\"response_result\"].cost)\n",
    "        print(r[\"response_result\"].chosen_action)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
