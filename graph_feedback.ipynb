{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | GPT3.5 | GPT4 |\n",
    "|----------|----------|----------|\n",
    "| GPT3.5   | 1      | 0      |\n",
    "| GPT4     | 1      | 1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# assuming LLM api keys have been set in the environment\n",
    "\n",
    "llm_35 = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature=0,\n",
    "    request_timeout=20,\n",
    "    max_retries=1,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm_4 = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4-32k\",\n",
    "    temperature=0,\n",
    "    request_timeout=20,\n",
    "    max_retries=1,\n",
    "    client=None,\n",
    ")\n",
    "\n",
    "llm_4.predict('What American cartoonist is the creator of Andy Lippincott?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_SIMPLE_PROMPT = \"\"\"{context}\"\"\"\n",
    "\n",
    "\n",
    "OTHER_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=_SIMPLE_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_feedback_chain import GraphFeedbackChain\n",
    "\n",
    "graph_chain = GraphFeedbackChain.from_llms({\"gpt-35-turbo\": llm_35, \"gpt-4\": llm_4}, prompt=OTHER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs={\"selected\": \"gpt-35-turbo\", \"context\":\"What American cartoonise is the creator of Andy Lippincott?\"}\n",
    "\n",
    "graph_chain.run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from response_validator import *\n",
    "\n",
    "template = \"PLEASE RESPOND ONLY WITH A SIGNLE FLOAT AND NO OTHER TEXT EXPLANATION\\n You are a VERY VERY strict judge that is called on to rank a response based on given criteria.\\\n",
    "                You must respond with your ranking by providing a single float within the range [-1, 1], -1 being very bad response and 1 being very good response.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Given the answer: {answer} to the question: {context}, how would you rate the answer of {llm_response}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "REWARD_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vw_example_builder import GraphFeedbackTextEmbedder\n",
    "import rl_chain\n",
    "\n",
    "chain = rl_chain.PickBest.from_chain(\n",
    "    llm_chain=graph_chain,\n",
    "    model_save_dir=\"./gf_models_actual_gf\",  # where to save the model checkpoints\n",
    "    response_validator = AutoValidatePickOne(llm=llm_35, prompt=REWARD_PROMPT),\n",
    "    vw_cmd=['--cb_explore_adf', '--quiet', '--interactions=::', '--coin', '--graph_feedback'],\n",
    "    prompt=OTHER_PROMPT,\n",
    "    text_embedder=GraphFeedbackTextEmbedder('bert-base-nli-mean-tokens'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = {\"actions\":[\"gpt-35-turbo\", \"gpt-4\"], \"graph\": \"0,0,1 0,1,0 1,0,1 1,1,1\"}\n",
    "\n",
    "for q, a in [(\"which country is john berry from\", \"United States of America country in North America\")]:\n",
    "    try:\n",
    "        inputs[\"context\"] = q\n",
    "        inputs[\"answer\"] = a\n",
    "        r = chain.run(inputs)\n",
    "        resp = r[\"response\"]\n",
    "        chosen_action = r[\"response_result\"].chosen_action\n",
    "        cost = r[\"response_result\"].cost\n",
    "        print(f\"m: {chosen_action} -- answer: {a} -- response: {resp}\")\n",
    "        print(r[\"response_result\"].cost)\n",
    "        print(r[\"response_result\"].chosen_action)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
